{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2files=\"/Users/ashishkumar/cs598dmcs/dataset/Hygiene/\"\n",
    "datFile = path2files+\"hygiene.dat\"\n",
    "labelFile = path2files+\"hygiene.dat.labels\"\n",
    "addDataFile = path2files + \"hygiene.dat.additional\"\n",
    "\n",
    "df1 = pd.read_csv(addDataFile,header=None)\n",
    "df1.columns = ['cuisine','zipCode','reviewCount','avgRating']\n",
    "df2 = pd.read_csv(datFile, sep='\\n',header=None)\n",
    "#only first 546 restaurants are labeled\n",
    "dataLabels = pd.read_csv(labelFile, sep='\\n',header=None)\n",
    "labeledData = 546\n",
    "lbl = dataLabels[0:labeledData][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 273, '1': 273})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashishkumar/anaconda3/envs/idp/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ashishkumar/anaconda3/envs/idp/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#df2.iloc[7][0]\n",
    "trainigdf = df1[0:labeledData]\n",
    "trainigdf['reviewText'] = df2[0:labeledData]\n",
    "trainigdf['lbl'] = lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "tokenizer.fit_on_texts(trainigdf['reviewText'])\n",
    "sequences = tokenizer.texts_to_sequences(trainigdf['reviewText'])\n",
    "data = pad_sequences(sequences, maxlen=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 273 samples, validate on 273 samples\n",
      "Epoch 1/5\n",
      "273/273 [==============================] - 101s 369ms/step - loss: 0.6935 - acc: 0.5092 - val_loss: 0.6929 - val_acc: 0.5128\n",
      "Epoch 2/5\n",
      "273/273 [==============================] - 95s 349ms/step - loss: 0.7050 - acc: 0.8132 - val_loss: 0.6949 - val_acc: 0.5092\n",
      "Epoch 3/5\n",
      "273/273 [==============================] - 92s 336ms/step - loss: 0.5682 - acc: 0.8498 - val_loss: 0.7032 - val_acc: 0.5018\n",
      "Epoch 4/5\n",
      "273/273 [==============================] - 92s 338ms/step - loss: 0.4074 - acc: 0.9048 - val_loss: 0.7725 - val_acc: 0.4908\n",
      "Epoch 5/5\n",
      "273/273 [==============================] - 94s 343ms/step - loss: 0.2119 - acc: 0.9707 - val_loss: 0.8641 - val_acc: 0.4908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa67a72550>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "In line two, we add an Embedding layer. This layer lets the network expand each token to a larger vector, \n",
    "allowing the network to represent words in a meaningful way. We pass 10000 as the first argument, \n",
    "which is the size of our vocabulary (remember, we told the tokenizer to only use the \n",
    "10000 most common words earlier), and 512 as the second, which means that each token can be \n",
    "expanded to a vector of size 512. We give it an input_length of 600, \n",
    "which is the length of each of our sequences.\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 512, input_length=600))\n",
    "model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "'''\n",
    "slow to train\n",
    "'''\n",
    "model.fit(data, np.array(trainigdf['lbl']), validation_split = 0.5, epochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 273 samples, validate on 273 samples\n",
      "Epoch 1/5\n",
      "273/273 [==============================] - 21s 77ms/step - loss: 0.6982 - acc: 0.4872 - val_loss: 0.6930 - val_acc: 0.5055\n",
      "Epoch 2/5\n",
      "273/273 [==============================] - 19s 70ms/step - loss: 0.6883 - acc: 0.6703 - val_loss: 0.6927 - val_acc: 0.5275\n",
      "Epoch 3/5\n",
      "273/273 [==============================] - 19s 69ms/step - loss: 0.6660 - acc: 0.8571 - val_loss: 0.7015 - val_acc: 0.4908\n",
      "Epoch 4/5\n",
      "273/273 [==============================] - 19s 68ms/step - loss: 0.5761 - acc: 0.8242 - val_loss: 0.6988 - val_acc: 0.5055\n",
      "Epoch 5/5\n",
      "273/273 [==============================] - 19s 71ms/step - loss: 0.5325 - acc: 0.9011 - val_loss: 0.6907 - val_acc: 0.5165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa712c9fd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "By adding a CNN layer before the LSTM, we allow the LSTM to see sequences of chunks instead of sequences of words. \n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 512, input_length=600))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(16, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(data, np.array(trainigdf['lbl']), validation_split = 0.5, epochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
