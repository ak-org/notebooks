{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Donor Finder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ak-org/notebooks/blob/master/Donor_Finder.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "OP-8JdYsVEh9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "1bc23c34-dafd-47a5-a257-28b18e7b24b1"
      },
      "cell_type": "code",
      "source": [
        "# Import libraries necessary for this project\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from IPython.display import display # Allows the use of display() for DataFrames\n",
        "import sklearn\n",
        "\n",
        "# Pretty display for notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the Census dataset\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header = None,\n",
        "                  names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
        "                           'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
        "\n",
        "# Success - Display the first record\n",
        "display(data.head(n=1))\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age   workclass  fnlwgt   education  education-num  marital-status  \\\n",
              "0   39   State-gov   77516   Bachelors             13   Never-married   \n",
              "\n",
              "      occupation    relationship    race    sex  capital-gain  capital-loss  \\\n",
              "0   Adm-clerical   Not-in-family   White   Male          2174             0   \n",
              "\n",
              "   hours-per-week  native-country  income  \n",
              "0              40   United-States   <=50K  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oBPvaELNh-JY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "60add7d7-97c5-4236-8aa1-15f88ace920c"
      },
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.3\r\n",
            "0.19.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R5yRgdw0X7d4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ab2a0172-d584-42dc-f43b-c48d556a2ef5"
      },
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
              "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
              "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
              "       'income'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "OgkRIoGxW7uz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Featureset Exploration**\n",
        "\n",
        "age: continuous.\n",
        "\n",
        "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "\n",
        "fnlwgt: continous\n",
        "\n",
        "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "\n",
        "education-num: continuous.\n",
        "\n",
        "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "\n",
        "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "\n",
        "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "\n",
        "race: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other.\n",
        "\n",
        "sex: Female, Male.\n",
        "\n",
        "capital-gain: continuous.\n",
        "\n",
        "capital-loss: continuous.\n",
        "\n",
        "hours-per-week: continuous.\n",
        "\n",
        "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
      ]
    },
    {
      "metadata": {
        "id": "Uc10DfFJV12n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "879f7e69-4424-4991-fcd9-6eb56246e3d8"
      },
      "cell_type": "code",
      "source": [
        "# Total number of records\n",
        "n_records = data.size\n",
        "\n",
        "# Number of records where individual's income is more than $50,000\n",
        "n_greater_50k = data[data['income'] != ' <=50K'].size\n",
        "\n",
        "# Number of records where individual's income is at most $50,000\n",
        "n_at_most_50k = data[data['income'] == ' <=50K'].size\n",
        "\n",
        "# Percentage of individuals whose income is more than $50,000\n",
        "greater_percent = (n_greater_50k/n_records) * 100\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of records: {}\".format(n_records))\n",
        "print(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\n",
        "print(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\n",
        "print(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percent))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of records: 488415\n",
            "Individuals making more than $50,000: 117615\n",
            "Individuals making at most $50,000: 370800\n",
            "Percentage of individuals making more than $50,000: 24.080955744602438%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6EHOjQBSWxm3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split the data into features and target label\n",
        "income_raw = data['income']\n",
        "features_raw = data.drop('income', axis = 1)\n",
        "\n",
        "# Log-transform the skewed features\n",
        "skewed = ['capital-gain', 'capital-loss']\n",
        "features_log_transformed = pd.DataFrame(data = features_raw)\n",
        "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWtPG_6HXm50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Normalizing Numerical Features**\n",
        "\n",
        "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below."
      ]
    },
    {
      "metadata": {
        "id": "A6rkDaBVXlJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1ceb971b-cdb8-4b67-9a2d-be77d7c89ad9"
      },
      "cell_type": "code",
      "source": [
        "# Import sklearn.preprocessing.StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize a scaler, then apply it to the features\n",
        "scaler = MinMaxScaler() # default=(0, 1)\n",
        "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
        "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
        "\n",
        "# Show an example of a record with scaling applied\n",
        "display(features_log_minmax_transform.head(n = 5))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.301370</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.667492</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.452055</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122449</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.493151</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>Cuba</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age          workclass  fnlwgt   education  education-num  \\\n",
              "0  0.301370          State-gov   77516   Bachelors       0.800000   \n",
              "1  0.452055   Self-emp-not-inc   83311   Bachelors       0.800000   \n",
              "2  0.287671            Private  215646     HS-grad       0.533333   \n",
              "3  0.493151            Private  234721        11th       0.400000   \n",
              "4  0.150685            Private  338409   Bachelors       0.800000   \n",
              "\n",
              "        marital-status          occupation    relationship    race      sex  \\\n",
              "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
              "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
              "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
              "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
              "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
              "\n",
              "   capital-gain  capital-loss  hours-per-week  native-country  \n",
              "0      0.667492           0.0        0.397959   United-States  \n",
              "1      0.000000           0.0        0.122449   United-States  \n",
              "2      0.000000           0.0        0.397959   United-States  \n",
              "3      0.000000           0.0        0.397959   United-States  \n",
              "4      0.000000           0.0        0.397959            Cuba  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MoSfheIhafPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d3d07224-d3a2-4708-a4a1-036300863d27"
      },
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
        "features_final = pd.get_dummies(features_log_minmax_transform)\n",
        "\n",
        "# Encode the 'income_raw' data to numerical values\n",
        "income = data['income'].astype('category').cat.codes\n",
        "\n",
        "# Print the number of features after one-hot encoding\n",
        "encoded = list(features_final.columns)\n",
        "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
        "\n",
        "# Uncomment the following line to see the encoded feature names\n",
        "print(encoded)\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108 total features after one-hot encoding.\n",
            "['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private', 'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc', 'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th', 'education_ 11th', 'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate', 'education_ HS-grad', 'education_ Masters', 'education_ Preschool', 'education_ Prof-school', 'education_ Some-college', 'marital-status_ Divorced', 'marital-status_ Married-AF-spouse', 'marital-status_ Married-civ-spouse', 'marital-status_ Married-spouse-absent', 'marital-status_ Never-married', 'marital-status_ Separated', 'marital-status_ Widowed', 'occupation_ ?', 'occupation_ Adm-clerical', 'occupation_ Armed-Forces', 'occupation_ Craft-repair', 'occupation_ Exec-managerial', 'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct', 'occupation_ Other-service', 'occupation_ Priv-house-serv', 'occupation_ Prof-specialty', 'occupation_ Protective-serv', 'occupation_ Sales', 'occupation_ Tech-support', 'occupation_ Transport-moving', 'relationship_ Husband', 'relationship_ Not-in-family', 'relationship_ Other-relative', 'relationship_ Own-child', 'relationship_ Unmarried', 'relationship_ Wife', 'race_ Amer-Indian-Eskimo', 'race_ Asian-Pac-Islander', 'race_ Black', 'race_ Other', 'race_ White', 'sex_ Female', 'sex_ Male', 'native-country_ ?', 'native-country_ Cambodia', 'native-country_ Canada', 'native-country_ China', 'native-country_ Columbia', 'native-country_ Cuba', 'native-country_ Dominican-Republic', 'native-country_ Ecuador', 'native-country_ El-Salvador', 'native-country_ England', 'native-country_ France', 'native-country_ Germany', 'native-country_ Greece', 'native-country_ Guatemala', 'native-country_ Haiti', 'native-country_ Holand-Netherlands', 'native-country_ Honduras', 'native-country_ Hong', 'native-country_ Hungary', 'native-country_ India', 'native-country_ Iran', 'native-country_ Ireland', 'native-country_ Italy', 'native-country_ Jamaica', 'native-country_ Japan', 'native-country_ Laos', 'native-country_ Mexico', 'native-country_ Nicaragua', 'native-country_ Outlying-US(Guam-USVI-etc)', 'native-country_ Peru', 'native-country_ Philippines', 'native-country_ Poland', 'native-country_ Portugal', 'native-country_ Puerto-Rico', 'native-country_ Scotland', 'native-country_ South', 'native-country_ Taiwan', 'native-country_ Thailand', 'native-country_ Trinadad&Tobago', 'native-country_ United-States', 'native-country_ Vietnam', 'native-country_ Yugoslavia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k9_YEpAOb0oE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ce0cf54-8322-4ca6-dbba-222193abc4f9"
      },
      "cell_type": "code",
      "source": [
        "# Import train_test_split\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import random\n",
        "\n",
        "# Split the 'features' and 'income' data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
        "                                                    income, \n",
        "                                                    test_size = 0.2, \n",
        "                                                    random_state = 0)\n",
        "\n",
        "\n",
        "# Show the results of the split\n",
        "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
        "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set has 26048 samples.\n",
            "Testing set has 6513 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gbAluqDKtJCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a841d589-2659-468a-f26d-fec825b95914"
      },
      "cell_type": "code",
      "source": [
        "!pip install prettytable"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prettytable\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Building wheels for collected packages: prettytable\n",
            "  Running setup.py bdist_wheel for prettytable ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "Successfully built prettytable\n",
            "Installing collected packages: prettytable\n",
            "Successfully installed prettytable-0.7.2\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VnLCckfGlOS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import fbeta_score, accuracy_score\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
        "    '''\n",
        "    inputs:\n",
        "       - learner: the learning algorithm to be trained and predicted on\n",
        "       - sample_size: the size of samples (number) to be drawn from training set\n",
        "       - X_train: features training set\n",
        "       - y_train: income training set\n",
        "       - X_test: features testing set\n",
        "       - y_test: income testing set\n",
        "    '''\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
        "    start = time() # Get start time\n",
        "    learner = learner.fit(X_train[0:sample_size], y_train[0:sample_size])\n",
        "    end = time() # Get end time\n",
        "    \n",
        "    # Calculate the training time\n",
        "    results['train_time'] = end - start\n",
        "        \n",
        "    # Get the predictions on the test set(X_test),\n",
        "    # then get predictions on the first 300 training samples(X_train) using .predict()\n",
        "    start = time() # Get start time\n",
        "    predictions_test = learner.predict(X_test)\n",
        "    predictions_train = learner.predict(X_train[0:300])\n",
        "    end = time() # Get end time\n",
        " \n",
        "    \n",
        "    # Calculate the total prediction time\n",
        "    results['pred_time'] = end - start\n",
        "            \n",
        "    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n",
        "    results['acc_train'] = accuracy_score(predictions_train, y_train[0:300])\n",
        "        \n",
        "    # TODO: Compute accuracy on test set using accuracy_score()\n",
        "    results['acc_test'] = accuracy_score(predictions_test, y_test)\n",
        "    \n",
        "    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n",
        "    results['f_train'] = fbeta_score(y_train[0:300], predictions_train, beta = 0.5)\n",
        "        \n",
        "    # TODO: Compute F-score on the test set which is y_test\n",
        "    results['f_test'] = fbeta_score(y_test, predictions_test, beta = 0.5)\n",
        "       \n",
        "    # Success\n",
        "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
        "    print(\"Train Acc : \", results['acc_train'])\n",
        "    print(\"Test Acc : \", results['acc_test'])    \n",
        "    print(\"F Train Acc : \", results['f_train'])\n",
        "    print(\"F Test Acc : \", results['f_test'])  \n",
        "        \n",
        "    # Return the results\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgWWftbklD9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "a1bec09b-40ab-4134-d39d-84faa23e65e5"
      },
      "cell_type": "code",
      "source": [
        "# Import the three supervised learning models from sklearn\n",
        "import sklearn\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "\n",
        "\n",
        "# Initialize the three models\n",
        "clf_A = AdaBoostClassifier()\n",
        "clf_B = BaggingClassifier()\n",
        "clf_C = GradientBoostingClassifier()\n",
        "\n",
        "# Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
        "# samples_100 is the entire training set i.e. len(y_train)\n",
        "# samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n",
        "# samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n",
        "samples_100 = len(y_train)\n",
        "samples_10 = int(0.10 * samples_100)\n",
        "samples_1 = int(0.01 * samples_100)\n",
        "\n",
        "# Collect results on the learners\n",
        "results = {}\n",
        "for clf in [clf_A, clf_B, clf_C]:\n",
        "    clf_name = clf.__class__.__name__\n",
        "    results[clf_name] = {}\n",
        "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
        "        results[clf_name][i] = \\\n",
        "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
        "\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier trained on 260 samples.\n",
            "Train Acc :  0.9166666666666666\n",
            "Test Acc :  0.7999385843697221\n",
            "F Train Acc :  0.842857142857143\n",
            "F Test Acc :  0.5926867659522944\n",
            "AdaBoostClassifier trained on 2604 samples.\n",
            "Train Acc :  0.88\n",
            "Test Acc :  0.8490710885920467\n",
            "F Train Acc :  0.7802547770700636\n",
            "F Test Acc :  0.6986055501863868\n",
            "AdaBoostClassifier trained on 26048 samples.\n",
            "Train Acc :  0.8766666666666667\n",
            "Test Acc :  0.855366190695532\n",
            "F Train Acc :  0.7781456953642384\n",
            "F Test Acc :  0.718159167521618\n",
            "BaggingClassifier trained on 260 samples.\n",
            "Train Acc :  0.9566666666666667\n",
            "Test Acc :  0.8257331490864425\n",
            "F Train Acc :  0.9217877094972068\n",
            "F Test Acc :  0.6496221257436887\n",
            "BaggingClassifier trained on 2604 samples.\n",
            "Train Acc :  0.99\n",
            "Test Acc :  0.8334101028711807\n",
            "F Train Acc :  0.9916201117318436\n",
            "F Test Acc :  0.6659790776484455\n",
            "BaggingClassifier trained on 26048 samples.\n",
            "Train Acc :  0.9833333333333333\n",
            "Test Acc :  0.8398587440503608\n",
            "F Train Acc :  0.9776536312849161\n",
            "F Test Acc :  0.6811849244757295\n",
            "GradientBoostingClassifier trained on 260 samples.\n",
            "Train Acc :  0.9566666666666667\n",
            "Test Acc :  0.8315676339628435\n",
            "F Train Acc :  0.9285714285714285\n",
            "F Test Acc :  0.658553076402975\n",
            "GradientBoostingClassifier trained on 2604 samples.\n",
            "Train Acc :  0.89\n",
            "Test Acc :  0.8575157377552587\n",
            "F Train Acc :  0.8163265306122449\n",
            "F Test Acc :  0.7283798365962694\n",
            "GradientBoostingClassifier trained on 26048 samples.\n",
            "Train Acc :  0.87\n",
            "Test Acc :  0.8608935974205435\n",
            "F Train Acc :  0.7653061224489796\n",
            "F Test Acc :  0.7332537669700134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LFaApAoUwkqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "ea56afe6-ac75-4b99-d055-bc8f1c628a0a"
      },
      "cell_type": "code",
      "source": [
        "## Model Tuning\n",
        "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
        "import sklearn\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import make_scorer, fbeta_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize the classifier\n",
        "clf = GradientBoostingClassifier()\n",
        "\n",
        "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
        "# parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\n",
        "parameters = {'learning_rate': [0.1, 0.01, 0.001],\n",
        "              'max_depth': [3,4,5,6]}\n",
        "\n",
        "# Make an fbeta_score scoring object using make_scorer()\n",
        "scorer = make_scorer(fbeta_score, beta = 0.5)\n",
        "\n",
        "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
        "grid_obj = GridSearchCV(estimator = clf, param_grid = parameters, scoring = scorer)\n",
        "\n",
        "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
        "grid_fit = grid_obj.fit(X_train, y_train)\n",
        "\n",
        "# Get the estimator\n",
        "best_clf = grid_fit.best_estimator_\n",
        "\n",
        "# Make predictions using the unoptimized and model\n",
        "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
        "best_predictions = best_clf.predict(X_test)\n",
        "\n",
        "# Report the before-and-afterscores\n",
        "print(\"Unoptimized model\\n------\")\n",
        "print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
        "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\n",
        "print(\"\\nOptimized Model\\n------\")\n",
        "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
        "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unoptimized model\n",
            "------\n",
            "Accuracy score on testing data: 0.8610\n",
            "F-score on testing data: 0.7337\n",
            "\n",
            "Optimized Model\n",
            "------\n",
            "Final accuracy score on the testing data: 0.8681\n",
            "Final F-score on the testing data: 0.7418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rXebatlS025N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "587e40ee-7867-4f1e-dc5c-33e893964f26"
      },
      "cell_type": "code",
      "source": [
        "# features importance - Can we identify top 5 significant features\n",
        "\n",
        "# Train the supervised model on the training set using .fit(X_train, y_train)\n",
        "model = clf.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Extract the feature importances using .feature_importances_ \n",
        "importances = model.feature_importances_\n",
        "\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature ranking:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-bec04c0b72da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d. feature %d (%f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'indices' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ocg07eKF2uLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1870
        },
        "outputId": "b58ac6d3-ae04-4389-c9c9-c452216063be"
      },
      "cell_type": "code",
      "source": [
        "indices = np.argsort(importances)[::-1]\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature ranking:\n",
            "1. feature 3 (0.139341)\n",
            "2. feature 0 (0.118915)\n",
            "3. feature 4 (0.114013)\n",
            "4. feature 2 (0.107263)\n",
            "5. feature 33 (0.101747)\n",
            "6. feature 5 (0.062423)\n",
            "7. feature 1 (0.037680)\n",
            "8. feature 58 (0.034124)\n",
            "9. feature 42 (0.029953)\n",
            "10. feature 46 (0.022890)\n",
            "11. feature 48 (0.020257)\n",
            "12. feature 43 (0.017904)\n",
            "13. feature 7 (0.017228)\n",
            "14. feature 51 (0.016284)\n",
            "15. feature 8 (0.015666)\n",
            "16. feature 12 (0.015034)\n",
            "17. feature 50 (0.012375)\n",
            "18. feature 49 (0.010666)\n",
            "19. feature 64 (0.009407)\n",
            "20. feature 44 (0.009364)\n",
            "21. feature 65 (0.009114)\n",
            "22. feature 11 (0.007088)\n",
            "23. feature 53 (0.006457)\n",
            "24. feature 45 (0.006302)\n",
            "25. feature 105 (0.005744)\n",
            "26. feature 32 (0.005232)\n",
            "27. feature 24 (0.004976)\n",
            "28. feature 35 (0.004609)\n",
            "29. feature 96 (0.004273)\n",
            "30. feature 10 (0.003640)\n",
            "31. feature 56 (0.003462)\n",
            "32. feature 36 (0.003391)\n",
            "33. feature 88 (0.003259)\n",
            "34. feature 68 (0.002741)\n",
            "35. feature 54 (0.002315)\n",
            "36. feature 38 (0.002314)\n",
            "37. feature 90 (0.001846)\n",
            "38. feature 67 (0.001518)\n",
            "39. feature 52 (0.001512)\n",
            "40. feature 79 (0.001463)\n",
            "41. feature 63 (0.001396)\n",
            "42. feature 22 (0.001208)\n",
            "43. feature 15 (0.000906)\n",
            "44. feature 66 (0.000893)\n",
            "45. feature 26 (0.000865)\n",
            "46. feature 57 (0.000788)\n",
            "47. feature 39 (0.000152)\n",
            "48. feature 31 (0.000000)\n",
            "49. feature 27 (0.000000)\n",
            "50. feature 23 (0.000000)\n",
            "51. feature 25 (0.000000)\n",
            "52. feature 30 (0.000000)\n",
            "53. feature 29 (0.000000)\n",
            "54. feature 28 (0.000000)\n",
            "55. feature 84 (0.000000)\n",
            "56. feature 21 (0.000000)\n",
            "57. feature 20 (0.000000)\n",
            "58. feature 104 (0.000000)\n",
            "59. feature 103 (0.000000)\n",
            "60. feature 102 (0.000000)\n",
            "61. feature 101 (0.000000)\n",
            "62. feature 6 (0.000000)\n",
            "63. feature 100 (0.000000)\n",
            "64. feature 99 (0.000000)\n",
            "65. feature 9 (0.000000)\n",
            "66. feature 98 (0.000000)\n",
            "67. feature 97 (0.000000)\n",
            "68. feature 13 (0.000000)\n",
            "69. feature 14 (0.000000)\n",
            "70. feature 16 (0.000000)\n",
            "71. feature 17 (0.000000)\n",
            "72. feature 18 (0.000000)\n",
            "73. feature 19 (0.000000)\n",
            "74. feature 95 (0.000000)\n",
            "75. feature 34 (0.000000)\n",
            "76. feature 74 (0.000000)\n",
            "77. feature 85 (0.000000)\n",
            "78. feature 69 (0.000000)\n",
            "79. feature 70 (0.000000)\n",
            "80. feature 71 (0.000000)\n",
            "81. feature 72 (0.000000)\n",
            "82. feature 73 (0.000000)\n",
            "83. feature 75 (0.000000)\n",
            "84. feature 37 (0.000000)\n",
            "85. feature 76 (0.000000)\n",
            "86. feature 77 (0.000000)\n",
            "87. feature 78 (0.000000)\n",
            "88. feature 80 (0.000000)\n",
            "89. feature 81 (0.000000)\n",
            "90. feature 82 (0.000000)\n",
            "91. feature 86 (0.000000)\n",
            "92. feature 62 (0.000000)\n",
            "93. feature 61 (0.000000)\n",
            "94. feature 60 (0.000000)\n",
            "95. feature 59 (0.000000)\n",
            "96. feature 87 (0.000000)\n",
            "97. feature 55 (0.000000)\n",
            "98. feature 106 (0.000000)\n",
            "99. feature 89 (0.000000)\n",
            "100. feature 91 (0.000000)\n",
            "101. feature 47 (0.000000)\n",
            "102. feature 92 (0.000000)\n",
            "103. feature 93 (0.000000)\n",
            "104. feature 94 (0.000000)\n",
            "105. feature 83 (0.000000)\n",
            "106. feature 41 (0.000000)\n",
            "107. feature 40 (0.000000)\n",
            "108. feature 107 (0.000000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NAZaB6U32_Ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "79488e15-91b4-41a6-e442-84a4e8ffcf1a"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(24, 8))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices],\n",
        "       color=\"r\", align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), indices)\n",
        "plt.xlim([-1, X_train.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABWQAAAHhCAYAAADpv7fJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1ca9504c50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MB1kBfl238Ee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "dc78344e-0f08-4aab-a68c-0238a50be2d3"
      },
      "cell_type": "code",
      "source": [
        "# Import functionality for cloning a model\n",
        "from sklearn.base import clone\n",
        "\n",
        "REDUCED_FEATURES = 10\n",
        "\n",
        "# Reduce the feature space\n",
        "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:REDUCED_FEATURES]]]\n",
        "X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:REDUCED_FEATURES]]]\n",
        "\n",
        "# Train on the \"best\" model found from grid search earlier\n",
        "clf = (clone(best_clf)).fit(X_train_reduced, y_train)\n",
        "\n",
        "# Make new predictions\n",
        "reduced_predictions = clf.predict(X_test_reduced)\n",
        "\n",
        "# Report scores from the final model using both versions of data\n",
        "print(\"Final Model trained on full data\\n------\")\n",
        "print(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
        "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n",
        "print(\"\\nFinal Model trained on reduced data\\n------\")\n",
        "print(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\n",
        "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Model trained on full data\n",
            "------\n",
            "Accuracy on testing data: 0.8681\n",
            "F-score on testing data: 0.7418\n",
            "\n",
            "Final Model trained on reduced data\n",
            "------\n",
            "Accuracy on testing data: 0.8638\n",
            "F-score on testing data: 0.7334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gbZA_Mbo4Swl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "We can achieve comparable F-Score on test data by using 10 most significant features as compared to  using all 108 features.\n",
        "\n",
        "Selecting prominent features saved us a significant training time without compromising on the accuracy of the model. \n"
      ]
    }
  ]
}